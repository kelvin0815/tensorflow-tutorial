{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import zipfile\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '.'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "This tutorial goes through a model which convert words into vectors called Word2vec.\n",
    "\n",
    "Word2vec is a group of related models that are used to produce word embeddings.Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
    "\n",
    "Reference:\n",
    "\n",
    "[Tensorflow: Vector Representations of Words](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "[Wikipedia: Word2vec](https://en.wikipedia.org/wiki/Word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    if not os.path.exists(DATA_DIR+filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url+filename, DATA_DIR+filename)\n",
    "    statinfo = os.stat(DATA_DIR+filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Found and verified', 'text8.zip')\n"
     ]
    }
   ],
   "source": [
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(DATA_DIR+filename) as f:\n",
    "        data = f.read(f.namelist()[0]).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Data size', 17005207)\n"
     ]
    }
   ],
   "source": [
    "words = read_data(filename)\n",
    "print('Data size', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build dictionary and replace rare words by UNK token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words, vocabulary_size):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict((v,k) for k,v in dictionary.iteritems())\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "            unk_count = 0\n",
    "        data.append(index)\n",
    "        count[0][1] = unk_count\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common words (+UNK)', [['UNK', 0], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)])\n",
      "Sample data\n",
      "[5239, 3084, 12, 6, 195, 2, 3137, 46]\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first']\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data')\n",
    "print(data[:8])\n",
    "print([reverse_dictionary[i] for i in data[:8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Function to generate a training batch for the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    assert batch_size % num_skips == 0\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span-1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3084, 'originated', '->', 5239, 'anarchism')\n",
      "(3084, 'originated', '->', 12, 'as')\n",
      "(12, 'as', '->', 3084, 'originated')\n",
      "(12, 'as', '->', 6, 'a')\n",
      "(6, 'a', '->', 12, 'as')\n",
      "(6, 'a', '->', 195, 'term')\n",
      "(195, 'term', '->', 6, 'a')\n",
      "(195, 'term', '->', 2, 'of')\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Skip-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "skip_window = 1\n",
    "num_skips = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_size = 8\n",
    "valid_window = 100\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # Vector Representation and its lookup\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\n",
    "        )\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        # weights and biases for NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [vocabulary_size, embedding_size], \n",
    "                stddev=1.0 / math.sqrt(embedding_size)\n",
    "            )\n",
    "        )\n",
    "        nce_biases = tf.Variable(\n",
    "            tf.zeros([vocabulary_size])\n",
    "        )\n",
    "        \n",
    "        # NCE loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_labels,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocabulary_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "        \n",
    "        # Cosine Similarity between valid examples and all embeddings\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset\n",
    "        )\n",
    "        similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(valid_dataset)\n",
    "        print [reverse_dictionary[i] for i in sess.run(valid_dataset)]\n",
    "        print sess.run(valid_embeddings).shape\n",
    "        print sess.run(normalized_embeddings).shape\n",
    "        print sess.run(similarity).shape\n",
    "        sess.close()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 282.45715332\n",
      "Nearest to from:  relapse, politeness, tumors, preeminence, gimp, required, sarcasm, superficial,\n",
      "Nearest to at:  wettest, gilmour, zimbabwean, rabbis, subtle, clements, ris, stocks,\n",
      "Nearest to an:  deterrent, rabat, constitute, salesman, gillies, disturb, eruptions, arte,\n",
      "Nearest to a:  lem, predominate, bauhaus, holliday, www, shareholders, kremlin, memorial,\n",
      "Nearest to be:  deutschen, emphasises, carpenters, cfl, dp, manna, votes, folding,\n",
      "Nearest to three:  adc, lexikon, comnena, alphonse, rockers, registries, interchangeability, immature,\n",
      "Nearest to american:  bsod, illustrious, hrer, germania, postmodernist, hasty, willi, jerseys,\n",
      "Nearest to during:  pompey, horst, trivia, burgos, mills, abbott, pear, sever,\n",
      "Average loss at step 2000: 113.853134915\n",
      "Average loss at step 4000: 52.7802186744\n",
      "Average loss at step 6000: 33.4228466823\n",
      "Average loss at step 8000: 23.2594624711\n",
      "Average loss at step 10000: 17.8283199575\n",
      "Nearest to from:  of, in, and, translates, sickness, tumors, gb, fricatives,\n",
      "Nearest to at:  in, indigenous, for, variations, to, rabbis, circumcision, and,\n",
      "Nearest to an:  the, apparent, extends, institution, rotating, beginning, a, airships,\n",
      "Nearest to a:  the, aberdeen, ethics, agave, this, UNK, decision, one,\n",
      "Nearest to be:  was, berlin, votes, mary, microscopy, wait, is, neutral,\n",
      "Nearest to three:  nine, one, aberdeen, eight, two, five, six, zero,\n",
      "Nearest to american:  hrer, victoriae, monopoly, nazi, history, exist, all, hasty,\n",
      "Nearest to during:  agave, pear, anxiety, wally, alabama, aberdeen, pads, identify,\n",
      "Average loss at step 12000: 14.0532509854\n",
      "Average loss at step 14000: 11.7959110502\n",
      "Average loss at step 16000: 9.88081061614\n",
      "Average loss at step 18000: 8.54101598179\n",
      "Average loss at step 20000: 8.11152797937\n",
      "Nearest to from:  of, in, and, at, baby, translates, on, for,\n",
      "Nearest to at:  in, for, and, dasyprocta, with, from, of, but,\n",
      "Nearest to an:  the, friars, thermodynamically, muhammad, apparent, operatorname, apologia, productivity,\n",
      "Nearest to a:  the, this, his, ethics, caiman, agouti, zurich, compatibility,\n",
      "Nearest to be:  was, have, is, faltered, byte, mary, by, microscopy,\n",
      "Nearest to three:  two, eight, five, nine, six, zero, one, four,\n",
      "Nearest to american:  and, monopoly, victoriae, illustrious, hrer, phoenix, history, miss,\n",
      "Nearest to during:  pear, and, in, anxiety, wally, islet, at, foam,\n",
      "Average loss at step 22000: 7.04892645442\n",
      "Average loss at step 24000: 6.75224976826\n",
      "Average loss at step 26000: 6.66355782807\n",
      "Average loss at step 28000: 6.36817984176\n",
      "Average loss at step 30000: 5.93250746405\n",
      "Nearest to from:  in, and, of, at, on, by, translates, seven,\n",
      "Nearest to at:  in, and, with, for, on, from, dasyprocta, during,\n",
      "Nearest to an:  the, thermodynamically, productivity, sponsors, trinomial, akihabara, apparent, azad,\n",
      "Nearest to a:  the, this, agouti, reuptake, caiman, ethics, compatibility, circ,\n",
      "Nearest to be:  have, was, is, by, faltered, mary, as, microscopy,\n",
      "Nearest to three:  two, five, four, six, eight, seven, nine, akihabara,\n",
      "Nearest to american:  and, bsod, acth, monopoly, illustrious, hasty, pendragon, heretic,\n",
      "Nearest to during:  in, at, pear, on, amalthea, wally, anxiety, mockery,\n",
      "Average loss at step 32000: 5.98737050271\n",
      "Average loss at step 34000: 5.69630970347\n",
      "Average loss at step 36000: 5.74907378352\n",
      "Average loss at step 38000: 5.50403449535\n",
      "Average loss at step 40000: 5.26623909545\n",
      "Nearest to from:  in, and, of, on, at, into, translates, baby,\n",
      "Nearest to at:  in, on, during, from, with, dasyprocta, for, but,\n",
      "Nearest to an:  thermodynamically, productivity, the, sponsors, ares, sapkowski, friars, azad,\n",
      "Nearest to a:  the, this, agouti, its, their, circ, eight, reuptake,\n",
      "Nearest to be:  have, was, is, by, faltered, were, been, mary,\n",
      "Nearest to three:  four, five, six, two, eight, seven, one, zero,\n",
      "Nearest to american:  bsod, and, illustrious, acth, jerseys, monopoly, congreso, history,\n",
      "Nearest to during:  in, at, pear, on, and, amalthea, mockery, until,\n",
      "Average loss at step 42000: 5.35426380742\n",
      "Average loss at step 44000: 5.2458447535\n",
      "Average loss at step 46000: 5.1981224426\n",
      "Average loss at step 48000: 5.22583403254\n",
      "Average loss at step 50000: 4.99842676294\n",
      "Nearest to from:  and, in, into, of, at, on, by, eight,\n",
      "Nearest to at:  in, during, on, near, from, with, dasyprocta, but,\n",
      "Nearest to an:  thermodynamically, the, bassoons, productivity, ares, sponsors, deterrent, azad,\n",
      "Nearest to a:  the, this, compatibility, kapoor, agouti, pairings, agave, no,\n",
      "Nearest to be:  have, is, was, by, been, were, faltered, are,\n",
      "Nearest to three:  four, five, six, two, seven, eight, one, akihabara,\n",
      "Nearest to american:  bsod, acth, illustrious, and, jerseys, monopoly, congreso, history,\n",
      "Nearest to during:  in, at, on, pear, and, of, until, wally,\n",
      "Average loss at step 52000: 5.060909621\n",
      "Average loss at step 54000: 5.20133343041\n",
      "Average loss at step 56000: 5.03891267157\n",
      "Average loss at step 58000: 5.05732852328\n",
      "Average loss at step 60000: 4.9459145968\n",
      "Nearest to from:  in, into, and, ursus, at, on, of, under,\n",
      "Nearest to at:  in, during, on, near, dasyprocta, from, wattle, but,\n",
      "Nearest to an:  thermodynamically, productivity, bassoons, deterrent, sponsors, dalits, eruptions, ares,\n",
      "Nearest to a:  the, compatibility, agouti, kapoor, any, reuptake, pairings, agave,\n",
      "Nearest to be:  have, been, was, by, faltered, were, is, are,\n",
      "Nearest to three:  five, four, six, two, seven, eight, ursus, akihabara,\n",
      "Nearest to american:  and, bsod, illustrious, history, acth, jerseys, monopoly, congreso,\n",
      "Nearest to during:  in, at, pear, after, on, until, from, baryogenesis,\n",
      "Average loss at step 62000: 5.00992327666\n",
      "Average loss at step 64000: 4.83755667168\n",
      "Average loss at step 66000: 4.58587427604\n",
      "Average loss at step 68000: 4.97829202676\n",
      "Average loss at step 70000: 4.88176838028\n",
      "Nearest to from:  into, in, ursus, under, on, at, fricatives, microcebus,\n",
      "Nearest to at:  in, during, mitral, microcebus, on, wattle, near, but,\n",
      "Nearest to an:  thermodynamically, deterrent, the, productivity, bassoons, azad, sponsors, ares,\n",
      "Nearest to a:  the, agouti, mitral, compatibility, agave, another, reuptake, any,\n",
      "Nearest to be:  been, have, are, by, is, were, was, not,\n",
      "Nearest to three:  six, five, four, two, eight, seven, nine, one,\n",
      "Nearest to american:  bsod, illustrious, history, congreso, jerseys, acth, british, and,\n",
      "Nearest to during:  in, at, after, pear, until, baryogenesis, from, wally,\n",
      "Average loss at step 72000: 4.75231254089\n",
      "Average loss at step 74000: 4.8056551671\n",
      "Average loss at step 76000: 4.72628603113\n",
      "Average loss at step 78000: 4.80153300983\n",
      "Average loss at step 80000: 4.79181449497\n",
      "Nearest to from:  into, in, under, on, ursus, of, at, during,\n",
      "Nearest to at:  in, during, mitral, near, microcebus, on, wattle, dasyprocta,\n",
      "Nearest to an:  thermodynamically, deterrent, bassoons, productivity, sponsors, azad, driftwood, ares,\n",
      "Nearest to a:  the, compatibility, kapoor, agouti, agave, ariane, cebus, busan,\n",
      "Nearest to be:  been, have, were, by, was, is, are, loyola,\n",
      "Nearest to three:  five, two, six, four, seven, eight, ursus, one,\n",
      "Nearest to american:  bsod, british, illustrious, congreso, history, acth, jerseys, bellarmine,\n",
      "Nearest to during:  in, at, after, until, of, pear, baryogenesis, when,\n",
      "Average loss at step 82000: 4.74814666045\n",
      "Average loss at step 84000: 4.75931929183\n",
      "Average loss at step 86000: 4.77605892146\n",
      "Average loss at step 88000: 4.7518038274\n",
      "Average loss at step 90000: 4.73182062805\n",
      "Nearest to from:  into, in, under, ursus, during, on, at, microcebus,\n",
      "Nearest to at:  in, during, mitral, microcebus, on, near, wattle, but,\n",
      "Nearest to an:  thermodynamically, deterrent, bassoons, productivity, rabat, ares, carney, peacocks,\n",
      "Nearest to a:  the, agouti, compatibility, mitral, kapoor, upanija, busan, cebus,\n",
      "Nearest to be:  been, have, were, was, by, is, are, not,\n",
      "Nearest to three:  four, five, two, seven, six, eight, one, ursus,\n",
      "Nearest to american:  british, bsod, illustrious, congreso, english, acth, history, bellarmine,\n",
      "Nearest to during:  in, at, after, while, when, from, until, under,\n",
      "Average loss at step 92000: 4.67329650068\n",
      "Average loss at step 94000: 4.72574374485\n",
      "Average loss at step 96000: 4.67673265469\n",
      "Average loss at step 98000: 4.58592908633\n",
      "Average loss at step 100000: 4.68409461725\n",
      "Nearest to from:  into, in, under, during, ursus, at, microcebus, eight,\n",
      "Nearest to at:  in, during, microcebus, near, mitral, wattle, on, under,\n",
      "Nearest to an:  thermodynamically, deterrent, bassoons, productivity, ares, carney, driftwood, eruptions,\n",
      "Nearest to a:  the, mitral, agouti, kapoor, another, ursus, agave, cebus,\n",
      "Nearest to be:  been, have, is, was, were, by, are, not,\n",
      "Nearest to three:  five, four, seven, two, six, eight, ursus, nine,\n",
      "Nearest to american:  british, bsod, illustrious, and, bellarmine, congreso, french, english,\n",
      "Nearest to during:  in, at, after, until, when, while, from, under,\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initialized')\n",
    "    \n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window\n",
    "        )\n",
    "        feed_dict = {\n",
    "            train_inputs: batch_inputs,\n",
    "            train_labels: batch_labels\n",
    "        }\n",
    "        \n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print('Average loss at step %s: %s' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1] # except self\n",
    "                log_str = \"Nearest to %s: \" % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Play & Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "\n",
    "def word_vector(word):\n",
    "    try:\n",
    "        vector = final_embeddings[dictionary[word]]\n",
    "        return vector\n",
    "    except KeyError:\n",
    "        return np.zeros(128)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (LA.norm(vec1) * LA.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_only = 500\n",
    "low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "plot_with_labels(low_dim_embs, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
